{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1118\n"
     ]
    }
   ],
   "source": [
    "from tinygrad.graph import print_tree\n",
    "from tinygrad.helpers import dtypes\n",
    "from tinygrad.jit import TinyJit\n",
    "from tinygrad.nn import Linear, Embedding\n",
    "from tinygrad.nn.optim import Adam\n",
    "from tinygrad.nn.state import get_parameters\n",
    "from tinygrad.ops  import ConstBuffer, UnaryOps, LoadOps\n",
    "from tinygrad.tensor import Tensor, Function\n",
    "from tinygrad.lazy import LazyBuffer\n",
    "from tinygrad.shape.shapetracker import ShapeTracker\n",
    "\n",
    "from tinygrad import Tensor\n",
    "from tinygrad.nn.state import get_parameters\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from data import iterate, maxX, maxY\n",
    "from model import RNNT, LSTM\n",
    "import numpy as np\n",
    "\n",
    "from trainplot import TrainPlot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y,B,C = 20,22,2,6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(x):\n",
    "    if isinstance(x,Tensor): x = x.numpy()\n",
    "    while len(x.shape) > 2: x = x[:,:,0]\n",
    "    plt.imshow(x[:,:])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsumexp(a:Tensor, b:Tensor):\n",
    "    mx = Tensor.maximum(a,b).maximum(-1e10)\n",
    "    s = (a-mx).exp() + (b-mx).exp()\n",
    "    return s.log() + mx\n",
    "inf = float('inf')\n",
    "\n",
    "def shear(d:Tensor,value = 0):\n",
    "    B,X,Y,C = d.shape\n",
    "    d = d.pad(((0,0),(0,Y),(0,0),(0,0)),value=value)\n",
    "    d = d.transpose(1,2).reshape((B,-1,C))\n",
    "    d = d[:,:(X+Y-1)*Y,:].realize()\n",
    "    return d.reshape((B,Y,X+Y-1,C)).transpose(1,2)\n",
    "\n",
    "def unshear(x:Tensor):\n",
    "    B,X,Y = x.shape\n",
    "    x = x.reshape((B,-1,))\n",
    "    x = x.pad(((0,0),(0,X),))\n",
    "    x = x.reshape((B,X,Y+1))\n",
    "    return x.shrink(((0,B),(0,X),(0,Y+1-X)))\n",
    "\n",
    "class TransducerLoss(Function):\n",
    "\n",
    "    def forward(self, d:Tensor, labels:Tensor):\n",
    "        self.B,self.X,self.Y,self.C = d.shape\n",
    "\n",
    "        self.labels = Tensor(labels).pad(((0,0),(0,1)))\n",
    "        self.lattice = shear(Tensor(d), 0.)\n",
    "        self.X = self.X+self.Y-1\n",
    "        assert self.lattice.shape == (self.B,self.X,self.Y,self.C), f\"{self.lattice.shape}\"\n",
    "\n",
    "        self.skip = shear(Tensor(d)[:,:,:,-1:],1.)[:,:,:,0].log()\n",
    "\n",
    "        self.p = self.lattice[\n",
    "            Tensor(np.arange(self.B).reshape((-1,1,1))),\n",
    "            Tensor(np.arange(self.X).reshape((1,-1,1))),\n",
    "            Tensor(np.arange(self.Y).reshape((1,1,-1))),\n",
    "            self.labels.reshape((self.B,1,-1))].log()\n",
    "\n",
    "        assert self.p.shape == (self.B, self.X, self.Y)\n",
    "        self.a = [Tensor([0]*self.B).reshape(-1,1).pad(((0,0),(0,self.Y-1),),-inf).realize()]\n",
    "\n",
    "        for x in range(0,self.X-1):\n",
    "            self.a.append(logsumexp(\n",
    "                (self.a[-1] + self.skip[:,x,:]).realize(),\n",
    "                (\n",
    "                    self.a[-1][:,:-1].pad(((0,0),(1,0),),-inf).realize() + self.p[:,x,:-1].pad(((0,0),(1,0),),-inf)\n",
    "                ).realize()\n",
    "            ))\n",
    "    \n",
    "        # return (-self.a[-1][:,-1] - self.skip[:,-1,-1]).sum().lazydata\n",
    "        return (-self.a[-1].max(1).sum()).lazydata\n",
    "    \n",
    "    def backward(self, g):\n",
    "\n",
    "        self.b = [None] * (self.X-1) + [Tensor.ones(self.B,self.Y)]\n",
    "        for x in range(self.X-2,-1,-1):\n",
    "            self.b[x] = (\n",
    "                logsumexp(\n",
    "                self.b[x+1] + self.skip[:,x,:],\n",
    "                self.b[x+1][:,1:].pad(((0,0),(0,1),),-inf).realize() + self.p[:,x,:].realize()\n",
    "             )).realize()\n",
    "\n",
    "        self.skg, self.p_grad = None, None\n",
    "\n",
    "        for a,b in zip(self.a[:-1], self.b[1:]):\n",
    "            sg = (a + b).reshape(self.B, 1,-1)\n",
    "            self.skg = sg if self.skg is None else self.skg.cat(sg,dim=1).realize()\n",
    "            pg = a.unsqueeze(1) + b[:,1:].pad(((0,0),(0,1),),-inf).unsqueeze(1)\n",
    "            self.p_grad = pg if self.p_grad is None else self.p_grad.cat(pg,dim=1).realize()\n",
    "\n",
    "        self.skg = (unshear(Tensor.cat(self.skg,(self.a[-1] + self.b[-1]).reshape(self.B, 1,-1),dim=1).realize().transpose(1,2)) - self.b[0][:,0].unsqueeze(1).unsqueeze(1)).exp().realize()\n",
    "        self.p_grad = (unshear(self.p_grad.pad(((0,0),(0,1),(0,0))).transpose(1,2)) +Tensor([1]*(self.Y-1) + [-inf]).unsqueeze(-1) - self.b[0][:,0].realize().unsqueeze(1).unsqueeze(1)).exp().realize()\n",
    "        self.p_grad = self.p_grad.unsqueeze(-1).mul(Tensor.eye(self.C-1)[self.labels].unsqueeze(2))\n",
    "\n",
    "        return (-Tensor.cat(self.p_grad,self.skg.unsqueeze(-1), dim=-1)).transpose(1,2).realize().lazydata,None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnnt = RNNT()\n",
    "opt = Adam(get_parameters(rnnt))\n",
    "X = Tensor.uniform(20,5,240)\n",
    "X_lens = Tensor.ones(5)*20\n",
    "Y = Tensor.randint(5,15,high=29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "i2c = list(\"abcdefghijklmnopqrstuvwxyz' \")+[\"<pad>\"]\n",
    "c2i = dict(map(reversed,enumerate(i2c)))\n",
    "C = len(i2c) # the last index stands for either the skip or pad. thesee are different.\n",
    "def text_encode(s):\n",
    "    if type(s[0]) == str: s = [s]\n",
    "    lens = list(map(len,s))\n",
    "    maxlen = max(lens)\n",
    "    encs = [list(map(c2i.__getitem__,e)) + (maxlen-l) * [c2i['<pad>']] for e,l in zip(s,lens)]\n",
    "    encs = np.array(encs)\n",
    "    return encs,lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask(d,X_lens, Y_lens, maxX, maxY):\n",
    "    d = d.pad(((0,0),(0,1),(0,0),(0,0)))\n",
    "    xrange = Tensor.arange(maxX+1)\n",
    "    mask = (xrange.unsqueeze(-1) < X_lens).T\n",
    "\n",
    "    d = d * mask.unsqueeze(-1).unsqueeze(-1)\n",
    "    mask = Tensor.arange\n",
    "\n",
    "    yrange = Tensor.arange(maxY + 1)\n",
    "    mask = (yrange.unsqueeze(-1).unsqueeze(-1)) < Y_lens\n",
    "    mask = mask.transpose(0,2)\n",
    "    d = d * mask.unsqueeze(-1)\n",
    "\n",
    "    line = (yrange.unsqueeze(0) == Y_lens.unsqueeze(-1)-1)\n",
    "    line = line.unsqueeze(1) * (xrange.unsqueeze(-1) >= X_lens.unsqueeze(1).unsqueeze(-1))\n",
    "    line = line.unsqueeze(3).pad(((0,0),(0,0),(0,0),(28,0)))\n",
    "\n",
    "    d = d + line\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maxX, maxY = (500, 276)\n",
    "# maxX, maxY = (333,184) # too big still\n",
    "rnnt = RNNT()\n",
    "opt = Adam(get_parameters(rnnt))\n",
    "iter = iterate(B)\n",
    "def step():\n",
    "    X,labels = next(iter)\n",
    "    X,X_lens = X\n",
    "    labels,Y_lens = text_encode(labels)\n",
    "    X_lens = Tensor(X_lens)\n",
    "    Y_lens = Tensor(Y_lens)\n",
    "    # print(X_lens.numpy(),Y_lens.numpy())\n",
    "    X = Tensor(X).pad(((0,maxX-X.shape[0]),(0,0),(0,0))).contiguous().realize()\n",
    "    labels = Tensor(labels).pad(((0,0),(0,maxY - labels.shape[1]))).contiguous().realize()\n",
    "    return fb_pass(X,labels,X_lens, Y_lens)/(X_lens.sum() + Y_lens.sum())\n",
    "@TinyJit\n",
    "def fb_pass(X:Tensor,labels, X_lens, Y_lens):\n",
    "    opt.zero_grad()\n",
    "    X = rnnt.encoder.__call__(X) # LSTM expects (N,B,D)\n",
    "    X_lens = (X_lens+1)/2\n",
    "\n",
    "    Y,_ = rnnt.prediction(labels,None,1)\n",
    "    Y = Y.pad(((0,0),(1,0),(0,0)))\n",
    "\n",
    "    d = rnnt.joint(X,Y).softmax(-1).realize()\n",
    "    md = mask(d,X_lens, Y_lens, maxX/2, maxY)\n",
    "\n",
    "    L = TransducerLoss.apply(md, labels)\n",
    "    L.backward()\n",
    "    opt.step()\n",
    "    return L.realize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 1118 samples in the dataset\n"
     ]
    }
   ],
   "source": [
    "L = step().numpy().item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "while True:\n",
    "    try:\n",
    "        L = L * 0.9 + step().numpy().item() * 0.1\n",
    "        plot(L=L)\n",
    "\n",
    "    except StopIteration:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter = iterate(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maxX, maxY = (500, 276)\n",
    "# maxX, maxY = (333,184) # too big still\n",
    "rnnt = RNNT()\n",
    "opt = Adam(get_parameters(rnnt))\n",
    "iter = iterate(B)\n",
    "def step():\n",
    "    X,labels = next(iter)\n",
    "    X,X_lens = X\n",
    "    labels,Y_lens = text_encode(labels)\n",
    "    X_lens = Tensor(X_lens)\n",
    "    Y_lens = Tensor(Y_lens)\n",
    "    # print(X_lens.numpy(),Y_lens.numpy())\n",
    "    X = Tensor(X).pad(((0,maxX-X.shape[0]),(0,0),(0,0))).contiguous().realize()\n",
    "    labels = Tensor(labels).pad(((0,0),(0,maxY - labels.shape[1]))).contiguous().realize()\n",
    "    return fb_pass(X,labels,X_lens, Y_lens)/(X_lens.sum() + Y_lens.sum())\n",
    "@TinyJit\n",
    "def fb_pass(X:Tensor,labels, X_lens, Y_lens):\n",
    "    global d\n",
    "    opt.zero_grad()\n",
    "    X = rnnt.encoder.__call__(X) # LSTM expects (N,B,D)\n",
    "    # X_lens = (X_lens+1)/2\n",
    "\n",
    "    Y,_ = rnnt.prediction(labels,None,1)\n",
    "    Y = Y.pad(((0,0),(1,0),(0,0)))\n",
    "\n",
    "    d = rnnt.joint(X,Y).softmax(-1).realize()\n",
    "\n",
    "    md = mask(d,X_lens, Y_lens, maxX/2, maxY)\n",
    "    L = TransducerLoss.apply(md, labels)\n",
    "\n",
    "    L.backward()\n",
    "    opt.step()\n",
    "    return L.realize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 5\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m      6\u001b[0m         L \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;28;01mif\u001b[39;00m L \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m L \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.9\u001b[39m \u001b[38;5;241m+\u001b[39m loss \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.1\u001b[39m\n\u001b[1;32m      7\u001b[0m         plot(L\u001b[38;5;241m=\u001b[39m L)\n",
      "File \u001b[0;32m~/code/dkormann/tinygrad/tinygrad/tensor.py:126\u001b[0m, in \u001b[0;36mTensor.numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m all_int(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno numpy if shape is symbolic, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mnp \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno numpy dtype for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_np\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCPU\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrealize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlazydata\u001b[38;5;241m.\u001b[39mrealized\u001b[38;5;241m.\u001b[39mtoCPU()\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mnp, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/code/dkormann/tinygrad/tinygrad/tensor.py:105\u001b[0m, in \u001b[0;36mTensor.realize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrealize\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 105\u001b[0m   \u001b[43mrun_schedule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlazydata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/code/dkormann/tinygrad/tinygrad/realize.py:34\u001b[0m, in \u001b[0;36mrun_schedule\u001b[0;34m(schedule, disable_logging)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m si\u001b[38;5;241m.\u001b[39mast\u001b[38;5;241m.\u001b[39mop \u001b[38;5;129;01min\u001b[39;00m LoadOps:\n\u001b[1;32m     32\u001b[0m   \u001b[38;5;66;03m# confirm the LoadOps are contiguous and in order\u001b[39;00m\n\u001b[1;32m     33\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m i,s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(si\u001b[38;5;241m.\u001b[39mast\u001b[38;5;241m.\u001b[39msrc): \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(s, LazyOp) \u001b[38;5;129;01mand\u001b[39;00m s\u001b[38;5;241m.\u001b[39mop \u001b[38;5;241m==\u001b[39m BufferOps\u001b[38;5;241m.\u001b[39mLOAD \u001b[38;5;129;01mand\u001b[39;00m s\u001b[38;5;241m.\u001b[39marg\u001b[38;5;241m.\u001b[39midx \u001b[38;5;241m==\u001b[39m i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m s\u001b[38;5;241m.\u001b[39marg\u001b[38;5;241m.\u001b[39mst\u001b[38;5;241m.\u001b[39mcontiguous, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbad LoadOps src \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 34\u001b[0m   \u001b[43mLOAD_OPS_DISPATCHER\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mLoadOps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43msi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     36\u001b[0m   \u001b[38;5;66;03m# TODO: should this be handled here? it probably just shouldn't be in the schedule\u001b[39;00m\n\u001b[1;32m     37\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(si\u001b[38;5;241m.\u001b[39mout\u001b[38;5;241m.\u001b[39mrealized, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m si\u001b[38;5;241m.\u001b[39mout\u001b[38;5;241m.\u001b[39mrealized\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/code/dkormann/tinygrad/tinygrad/realize.py:70\u001b[0m, in \u001b[0;36m_realize_from\u001b[0;34m(buffer, src)\u001b[0m\n\u001b[1;32m     68\u001b[0m   buffer\u001b[38;5;241m.\u001b[39mrealized\u001b[38;5;241m.\u001b[39m_transfer(src\u001b[38;5;241m.\u001b[39mrealized)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 70\u001b[0m   buffer\u001b[38;5;241m.\u001b[39mrealized\u001b[38;5;241m.\u001b[39m_copyin(\u001b[43msrc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrealized\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoCPU\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/code/dkormann/tinygrad/tinygrad/runtime/lib.py:47\u001b[0m, in \u001b[0;36mRawBufferCopyInOut.toCPU\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtoCPU\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m     46\u001b[0m   x: np\u001b[38;5;241m.\u001b[39mndarray \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mnp)\n\u001b[0;32m---> 47\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_copyout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/code/dkormann/tinygrad/tinygrad/runtime/ops_gpu.py:61\u001b[0m, in \u001b[0;36mCLBuffer._copyout\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     59\u001b[0m buf \u001b[38;5;241m=\u001b[39m cl\u001b[38;5;241m.\u001b[39mBuffer(CL\u001b[38;5;241m.\u001b[39mcl_ctxs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buf\u001b[38;5;241m.\u001b[39mdevice], cl\u001b[38;5;241m.\u001b[39mmem_flags\u001b[38;5;241m.\u001b[39mWRITE_ONLY \u001b[38;5;241m|\u001b[39m cl\u001b[38;5;241m.\u001b[39mmem_flags\u001b[38;5;241m.\u001b[39mUSE_HOST_PTR, \u001b[38;5;241m0\u001b[39m, hostbuf\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdata)\n\u001b[1;32m     60\u001b[0m mapped, event \u001b[38;5;241m=\u001b[39m cl\u001b[38;5;241m.\u001b[39menqueue_map_buffer(CL\u001b[38;5;241m.\u001b[39mcl_queue[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buf\u001b[38;5;241m.\u001b[39mdevice], buf, cl\u001b[38;5;241m.\u001b[39mmap_flags\u001b[38;5;241m.\u001b[39mWRITE, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mnp, is_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 61\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m mapped\u001b[38;5;241m.\u001b[39mbase: \u001b[43mcl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menqueue_copy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCL\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcl_queue\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_buf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_buf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait_for\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mevent\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mevt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mevt\u001b[49m\u001b[38;5;241;43m:=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mevent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/develop/lib/python3.11/site-packages/pyopencl/__init__.py:2014\u001b[0m, in \u001b[0;36menqueue_copy\u001b[0;34m(queue, dest, src, **kwargs)\u001b[0m\n\u001b[1;32m   2007\u001b[0m             warn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice_offset\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m argument of enqueue_copy \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2008\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis deprecated. Use \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msrc_offset\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m instead. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2009\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdst_offset\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m will stop working in 2023.x.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2010\u001b[0m                     \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m   2012\u001b[0m             kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msrc_offset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m device_offset\n\u001b[0;32m-> 2014\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_cl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_enqueue_read_buffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2016\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m src\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;129;01min\u001b[39;00m _IMAGE_MEM_OBJ_TYPES:\n\u001b[1;32m   2017\u001b[0m     origin \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morigin\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "L = None\n",
    "while True:\n",
    "    \n",
    "    try:\n",
    "        loss = step().numpy().item()\n",
    "        L = loss if L == None else L * 0.9 + loss * 0.1\n",
    "        plot(L= L)\n",
    "\n",
    "    except StopIteration:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d2f5c32e34649d89c0c46a1bb275e66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(height='4.952in', overflow='hidden'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot = TrainPlot()\n",
    "plot.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "develop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
